{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOah6GYI2L3lSa2F5eUVHLh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"V86l1womKsqb"},"outputs":[],"source":["from google.colab import drive"]},{"cell_type":"code","source":["drive.mount('/content/drive')"],"metadata":{"id":"etkjAU_xK8b2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736613581820,"user_tz":-120,"elapsed":38773,"user":{"displayName":"Franco Ryan","userId":"01207996925900706828"}},"outputId":"92d7715d-4be0-42bb-fb6f-28da9fc189cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PJBStFM3Ni1l","executionInfo":{"status":"ok","timestamp":1736613581820,"user_tz":-120,"elapsed":7,"user":{"displayName":"Franco Ryan","userId":"01207996925900706828"}},"outputId":"7c705492-2844-4781-e323-727c5d7adc55"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"]}]},{"cell_type":"code","source":["cd drive/MyDrive/Translation2/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dPhqmtDxNlDA","executionInfo":{"status":"ok","timestamp":1736613581820,"user_tz":-120,"elapsed":5,"user":{"displayName":"Franco Ryan","userId":"01207996925900706828"}},"outputId":"14f3d7f4-2c2c-4e7b-b6b5-5f9a0af755bf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Translation2\n"]}]},{"cell_type":"code","source":["ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0r8wK_G8NqgR","executionInfo":{"status":"ok","timestamp":1736613581821,"user_tz":-120,"elapsed":4,"user":{"displayName":"Franco Ryan","userId":"01207996925900706828"}},"outputId":"b6aeb006-f3af-42f0-c9f2-2875587c6811"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0m\u001b[01;34mData\u001b[0m/          metrics_history.json  \u001b[01;34m__pycache__\u001b[0m/   train.py            utilities.py\n","dictionary.py  models.py             \u001b[01;34msaved_models\u001b[0m/  Translation2.ipynb\n"]}]},{"cell_type":"code","source":["!pip install torch transformers datasets sacrebleu nltk wandb sentencepiece"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XjenDsbnNzTd","executionInfo":{"status":"ok","timestamp":1736613594607,"user_tz":-120,"elapsed":5978,"user":{"displayName":"Franco Ryan","userId":"01207996925900706828"}},"outputId":"b908de66-49e7-4e08-9010-c7ac913dd352"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n","Collecting datasets\n","  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n","Collecting sacrebleu\n","  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n","Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.1)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec (from torch)\n","  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.11)\n","Collecting portalocker (from sacrebleu)\n","  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n","Collecting colorama (from sacrebleu)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (5.3.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n","Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.44)\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n","Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n","Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.10.4)\n","Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n","Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n","Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, portalocker, fsspec, dill, colorama, sacrebleu, multiprocess, datasets\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2024.10.0\n","    Uninstalling fsspec-2024.10.0:\n","      Successfully uninstalled fsspec-2024.10.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed colorama-0.4.6 datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 portalocker-3.1.1 sacrebleu-2.5.1 xxhash-3.5.0\n"]}]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')  # optional\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SYo7A7wrYhuY","executionInfo":{"status":"ok","timestamp":1736613600925,"user_tz":-120,"elapsed":6320,"user":{"displayName":"Franco Ryan","userId":"01207996925900706828"}},"outputId":"1e58604c-1706-464f-cfb1-82176236e05d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["%%writefile train.py\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import argparse\n","import time\n","import os\n","import json\n","import pickle\n","import numpy as np\n","from random import shuffle\n","from models import Encoder, Decoder, Transformer\n","from utilities import load_files, load_batches, tokenize\n","\n","# Additional imports for metrics\n","from sacrebleu.metrics import BLEU, CHRF\n","import nltk\n","from nltk.translate.meteor_score import meteor_score\n","nltk.download('punkt')\n","\n","PAD_TOKEN = 0\n","SOS_TOKEN = 1\n","EOS_TOKEN = 2\n","\n","class MetricsLogger:\n","    def __init__(self, output_path=\"metrics_history.json\"):\n","        self.output_path = output_path\n","        self.metrics_history = []\n","\n","    def log_epoch_metrics(self, epoch, metrics_dict):\n","        entry = {\"epoch\": epoch}\n","        entry.update(metrics_dict)\n","        self.metrics_history.append(entry)\n","\n","    def save(self):\n","        with open(self.output_path, \"w\") as f:\n","            json.dump(self.metrics_history, f, indent=2)\n","\n","def compute_metrics(predictions, references):\n","    bleu_metric = BLEU()\n","    chrf_metric = CHRF()\n","\n","    bleu_result = bleu_metric.corpus_score(predictions, [references])\n","    chrf_result = chrf_metric.corpus_score(predictions, [references])\n","\n","    # METEOR\n","    meteor_vals = []\n","    for hyp, ref in zip(predictions, references):\n","        hyp_tokens = hyp.split()\n","        ref_tokens = ref.split()\n","        meteor_vals.append(meteor_score([ref_tokens], hyp_tokens))\n","    meteor_avg = 100 * np.mean(meteor_vals)\n","\n","    return {\n","        \"bleu\": bleu_result.score,\n","        \"chrf\": chrf_result.score,\n","        \"meteor\": meteor_avg\n","    }\n","\n","def ids_to_string(ids_list, dictionary):\n","    \"\"\"\n","    Utility to convert a list of token IDs into a single string, ignoring special tokens.\n","    dictionary.index2word is assumed to map index -> token.\n","    \"\"\"\n","    words = []\n","    for tok_id in ids_list:\n","        if tok_id in [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN]:\n","            continue\n","        words.append(dictionary.index2word[tok_id])\n","    return \" \".join(words)\n","\n","def greedy_decode(model, src, src_mask, max_len=60, start_symbol=SOS_TOKEN):\n","    model.eval()  # set model to evaluation mode\n","    memory = model.encoder(src, src_mask)\n","    ys = torch.ones(src.size(0), 1, dtype=torch.long, device=model.device) * start_symbol\n","\n","    for i in range(max_len - 1):\n","        target_mask = model.make_target_mask(ys)\n","        out, _ = model.decoder(ys, memory, target_mask, src_mask)\n","        prob = out[:, -1, :]\n","        next_word = torch.argmax(prob, dim=1).unsqueeze(1)\n","        ys = torch.cat([ys, next_word], dim=1)\n","    return ys\n","\n","class Trainer:\n","    def initialize_weights(self, model):\n","        if hasattr(model, 'weight') and model.weight.dim() > 1:\n","            nn.init.xavier_uniform_(model.weight.data)\n","\n","    def save_dictionary(self, dictionary, input=True):\n","        directory = f'saved_models/{self.input_lang_dic.name}2{self.output_lang_dic.name}/'\n","        os.makedirs(directory, exist_ok=True)\n","        file_path = directory + ('input_dic.pkl' if input else 'output_dic.pkl')\n","        with open(file_path, 'wb') as f:\n","            pickle.dump(dictionary, f, pickle.HIGHEST_PROTOCOL)\n","\n","    def __init__(self, lang1, lang2, data_directory, reverse, MAX_LENGTH, MAX_FILE_SIZE, batch_size, lr=0.0005,\n","                 hidden_size=256, encoder_layers=3, decoder_layers=3, encoder_heads=8, decoder_heads=8,\n","                 encoder_ff_size=512, decoder_ff_size=512, encoder_dropout=0.1, decoder_dropout=0.1,\n","                 device='cpu', lr_scheduler_type='linear', warmup_steps=500, early_stopping_patience=2):\n","        \"\"\"\n","        Extended constructor to accept the same hyperparams as in mbart_train.py, e.g.:\n","         - lr_scheduler_type\n","         - warmup_steps\n","         - early_stopping_patience\n","        \"\"\"\n","        self.MAX_LENGTH = MAX_LENGTH\n","        self.MAX_FILE_SIZE = MAX_FILE_SIZE\n","        self.device = device\n","        self.lr_scheduler_type = lr_scheduler_type\n","        self.warmup_steps = warmup_steps\n","        self.early_stopping_patience = early_stopping_patience\n","\n","        if reverse:\n","            lang1, lang2 = lang2, lang1\n","\n","        # ====================\n","        # Load raw sentences\n","        # ====================\n","        self.input_lang_dic, self.output_lang_dic, self.input_lang_list, self.output_lang_list = load_files(\n","            lang1, lang2, data_directory, reverse, self.MAX_FILE_SIZE, self.MAX_LENGTH)\n","\n","        # Add them to dictionary\n","        for sentence in self.input_lang_list:\n","            self.input_lang_dic.add_sentence(sentence)\n","        for sentence in self.output_lang_list:\n","            self.output_lang_dic.add_sentence(sentence)\n","\n","        # Save dictionary\n","        self.save_dictionary(self.input_lang_dic, input=True)\n","        self.save_dictionary(self.output_lang_dic, input=False)\n","\n","        # ====================\n","        # Tokenize entire data\n","        # ====================\n","        self.tokenized_input_lang = [\n","            tokenize(sentence, self.input_lang_dic, self.MAX_LENGTH)\n","            for sentence in self.input_lang_list\n","        ]\n","        self.tokenized_output_lang = [\n","            tokenize(sentence, self.output_lang_dic, self.MAX_LENGTH)\n","            for sentence in self.output_lang_list\n","        ]\n","\n","        # This was your original single data loader, we keep it (do not remove):\n","        self.batch_size = batch_size\n","        self.data_loader = load_batches(\n","            self.tokenized_input_lang,\n","            self.tokenized_output_lang,\n","            self.batch_size,\n","            self.device\n","        )\n","\n","        # ====================\n","        # NEW: 70/15/15 Split\n","        # ====================\n","        # Combine inputs/outputs into pairs so we can shuffle them together\n","        combined = list(zip(self.tokenized_input_lang, self.tokenized_output_lang))\n","        shuffle(combined)  # Shuffle pairs in-place\n","\n","        total_count = len(combined)\n","        train_end = int(0.70 * total_count)  # 70%\n","        valid_end = int(0.85 * total_count)  # 15% after train\n","        # test_end = total_count (implicitly 15%)\n","\n","        train_pairs = combined[:train_end]\n","        valid_pairs = combined[train_end:valid_end]\n","        test_pairs  = combined[valid_end:]\n","\n","        # Separate them back into inputs / outputs\n","        train_input, train_output = zip(*train_pairs) if train_pairs else ([], [])\n","        valid_input, valid_output = zip(*valid_pairs) if valid_pairs else ([], [])\n","        test_input,  test_output  = zip(*test_pairs)  if test_pairs  else ([], [])\n","\n","        # Now create separate data loaders\n","        self.train_loader = load_batches(train_input, train_output, self.batch_size, self.device)\n","        self.valid_loader = load_batches(valid_input, valid_output, self.batch_size, self.device)\n","        self.test_loader  = load_batches(test_input,  test_output,  self.batch_size, self.device)\n","\n","        # ====================\n","        # Build the model\n","        # ====================\n","        input_size = self.input_lang_dic.n_count\n","        output_size = self.output_lang_dic.n_count\n","\n","        encoder_part = Encoder(\n","            input_size, hidden_size, encoder_layers, encoder_heads,\n","            encoder_ff_size, encoder_dropout, self.device\n","        )\n","        decoder_part = Decoder(\n","            output_size, hidden_size, decoder_layers, decoder_heads,\n","            decoder_ff_size, decoder_dropout, self.device\n","        )\n","\n","        self.transformer = Transformer(encoder_part, decoder_part, self.device, PAD_TOKEN).to(self.device)\n","        self.transformer.apply(self.initialize_weights)\n","\n","        self.loss_func = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n","        self.optimizer = optim.Adam(self.transformer.parameters(), lr=lr)\n","        # self.scheduler = ...  (if needed)\n","\n","    def train(self, epochs, saved_model_directory):\n","        start_time = time.time()\n","        best_loss = float('inf')\n","        epochs_no_improve = 0\n","\n","        # Initialize the metrics logger\n","        logger = MetricsLogger(output_path=\"metrics_history.json\")\n","\n","        for epoch in range(epochs):\n","            # =====================================\n","            # 1) Training loop on TRAIN LOADER\n","            # =====================================\n","            shuffle(self.train_loader)  # shuffle the training batches each epoch (if you want)\n","            train_loss = 0.0\n","            self.transformer.train()\n","\n","            for input_batch, target_batch in self.train_loader:\n","                self.optimizer.zero_grad()\n","\n","                output, _ = self.transformer(input_batch, target_batch[:, :-1])\n","                output_dim = output.shape[-1]\n","                output = output.contiguous().view(-1, output_dim)\n","                target_flat = target_batch[:, 1:].contiguous().view(-1)\n","\n","                loss = self.loss_func(output, target_flat)\n","                loss.backward()\n","                nn.utils.clip_grad_norm_(self.transformer.parameters(), 1)\n","                self.optimizer.step()\n","\n","                train_loss += loss.item()\n","\n","            train_loss /= len(self.train_loader) if len(self.train_loader) > 0 else 1\n","\n","            # =====================================\n","            # 2) Evaluation on VALID LOADER\n","            # =====================================\n","            preds_text, refs_text = [], []\n","            val_loss = 0.0\n","            self.transformer.eval()\n","            with torch.no_grad():\n","                for input_batch, target_batch in self.valid_loader:\n","                    # Forward pass just to compute validation loss\n","                    output, _ = self.transformer(input_batch, target_batch[:, :-1])\n","                    output_dim = output.shape[-1]\n","                    output = output.contiguous().view(-1, output_dim)\n","                    target_flat = target_batch[:, 1:].contiguous().view(-1)\n","                    loss = self.loss_func(output, target_flat)\n","                    val_loss += loss.item()\n","\n","                    # For metrics: decode predictions and compare with references\n","                    input_mask = self.transformer.make_input_mask(input_batch)\n","                    decoded_output = greedy_decode(\n","                        self.transformer, input_batch, input_mask, max_len=self.MAX_LENGTH\n","                    )\n","                    for i in range(input_batch.size(0)):\n","                        pred_string = ids_to_string(decoded_output[i].tolist(), self.output_lang_dic)\n","                        ref_string  = ids_to_string(target_batch[i].tolist(),  self.output_lang_dic)\n","                        preds_text.append(pred_string)\n","                        refs_text.append(ref_string)\n","\n","            val_loss /= len(self.valid_loader) if len(self.valid_loader) > 0 else 1\n","            metric_results = compute_metrics(preds_text, refs_text)\n","\n","            # =====================================\n","            # 3) Save model checkpoint\n","            # =====================================\n","            model_directory = f\"{saved_model_directory}/{self.input_lang_dic.name}2{self.output_lang_dic.name}/\"\n","            os.makedirs(model_directory, exist_ok=True)\n","\n","            epoch_model_path = os.path.join(model_directory, f\"transformer_model_epoch_{epoch}.pt\")\n","            torch.save(self.transformer.state_dict(), epoch_model_path)\n","\n","            config = {\n","                \"model_type\": \"transformer\",\n","                \"hidden_size\": 256,\n","                \"num_attention_heads\": 8,\n","                \"num_hidden_layers\": 3,\n","                \"vocab_size\": self.input_lang_dic.n_count,\n","                \"max_position_embeddings\": self.MAX_LENGTH,\n","                \"hidden_dropout_prob\": 0.1\n","            }\n","            with open(os.path.join(model_directory, 'config.json'), 'w') as f:\n","                json.dump(config, f)\n","\n","            # =====================================\n","            # 4) Logging\n","            # =====================================\n","            epoch_time = int(time.time() - start_time)\n","            remaining_time = (epochs - epoch - 1) * epoch_time\n","            print(f\"Epoch: {epoch}, Time: {epoch_time}s, Estimated {remaining_time} sec remaining.\")\n","            print(f\"\\tTraining Loss: {train_loss:.4f} | Validation Loss: {val_loss:.4f}\")\n","            print(f\"\\tBLEU: {metric_results['bleu']:.2f}, CHRF: {metric_results['chrf']:.2f}, METEOR: {metric_results['meteor']:.2f}\\n\")\n","\n","            # Log to JSON\n","            logger.log_epoch_metrics(epoch, {\n","                \"train_loss\": train_loss,\n","                \"val_loss\": val_loss,\n","                \"bleu\": metric_results[\"bleu\"],\n","                \"chrf\": metric_results[\"chrf\"],\n","                \"meteor\": metric_results[\"meteor\"]\n","            })\n","\n","            # =====================================\n","            # 5) Early stopping on val_loss\n","            # =====================================\n","            if val_loss < best_loss:\n","                best_loss = val_loss\n","                epochs_no_improve = 0\n","            else:\n","                epochs_no_improve += 1\n","                if epochs_no_improve >= self.early_stopping_patience:\n","                    print(\"Early stopping triggered.\")\n","                    break\n","\n","        logger.save()\n","        print('Training finished!')\n","\n","        # ======================================\n","        # (Optional) Final Test Evaluation\n","        # ======================================\n","        if len(self.test_loader) > 0:\n","            print(\"Evaluating on TEST set...\")\n","            preds_text, refs_text = [], []\n","            test_loss = 0.0\n","            self.transformer.eval()\n","            with torch.no_grad():\n","                for input_batch, target_batch in self.test_loader:\n","                    # Forward pass for test loss\n","                    output, _ = self.transformer(input_batch, target_batch[:, :-1])\n","                    output_dim = output.shape[-1]\n","                    output = output.contiguous().view(-1, output_dim)\n","                    target_flat = target_batch[:, 1:].contiguous().view(-1)\n","                    loss = self.loss_func(output, target_flat)\n","                    test_loss += loss.item()\n","\n","                    # Decode for metrics\n","                    input_mask = self.transformer.make_input_mask(input_batch)\n","                    decoded_output = greedy_decode(self.transformer, input_batch, input_mask, max_len=self.MAX_LENGTH)\n","                    for i in range(input_batch.size(0)):\n","                        pred_string = ids_to_string(decoded_output[i].tolist(), self.output_lang_dic)\n","                        ref_string  = ids_to_string(target_batch[i].tolist(),  self.output_lang_dic)\n","                        preds_text.append(pred_string)\n","                        refs_text.append(ref_string)\n","\n","            test_loss /= len(self.test_loader)\n","            test_metrics = compute_metrics(preds_text, refs_text)\n","            print(f\"TEST Loss: {test_loss:.4f}\")\n","            print(f\"TEST BLEU: {test_metrics['bleu']:.2f}, TEST CHRF: {test_metrics['chrf']:.2f}, TEST METEOR: {test_metrics['meteor']:.2f}\")\n","\n","\n","def main():\n","    parser = argparse.ArgumentParser(description='Hyperparameters for training Transformer')\n","    parser.add_argument('--lang1', type=str, default='french', help='first language in language text file')\n","    parser.add_argument('--lang2', type=str, default='english', help='second language in language text file')\n","    parser.add_argument('--data_directory', type=str, default='data', help='data directory')\n","    parser.add_argument('--reverse', type=int, default=0, help='whether to switch roles of lang1 and lang2 as input/output')\n","    parser.add_argument('--MAX_LENGTH', type=int, default=60, help='max number of tokens in input')\n","    parser.add_argument('--MAX_FILE_SIZE', type=int, default=100000, help='max lines to read from files')\n","    parser.add_argument('--batch_size', type=int, default=128, help='batch size')\n","    parser.add_argument('--lr', type=float, default=0.0005, help='learning rate')\n","    parser.add_argument('--hidden_size', type=int, default=256, help='transformer hidden size')\n","    parser.add_argument('--encoder_layers', type=int, default=3, help='number of encoder layers')\n","    parser.add_argument('--decoder_layers', type=int, default=3, help='number of decoder layers')\n","    parser.add_argument('--encoder_heads', type=int, default=8, help='encoder attention heads')\n","    parser.add_argument('--decoder_heads', type=int, default=8, help='decoder attention heads')\n","    parser.add_argument('--encoder_ff_size', type=int, default=512, help='encoder FF size')\n","    parser.add_argument('--decoder_ff_size', type=int, default=512, help='decoder FF size')\n","    parser.add_argument('--encoder_dropout', type=float, default=0.1, help='encoder dropout')\n","    parser.add_argument('--decoder_dropout', type=float, default=0.1, help='decoder dropout')\n","    parser.add_argument('--device', type=str, default='cpu', help='device: cpu or cuda')\n","    parser.add_argument('--epochs', type=int, default=50, help='training epochs')\n","    parser.add_argument('--saved_model_directory', type=str, default='saved_models/', help='directory for saving models')\n","\n","    # Additional arguments for hyperparameter tuning, consistent with mbart_train.py\n","    parser.add_argument('--lr_scheduler_type', type=str, default='linear', help='LR scheduler type')\n","    parser.add_argument('--warmup_steps', type=int, default=500, help='Warmup steps for LR scheduler')\n","    parser.add_argument('--early_stopping_patience', type=int, default=2, help='Epochs with no improvement for early stopping')\n","\n","    args = parser.parse_args()\n","\n","    trainer = Trainer(\n","        lang1=args.lang1,\n","        lang2=args.lang2,\n","        data_directory=args.data_directory,\n","        reverse=args.reverse,\n","        MAX_LENGTH=args.MAX_LENGTH,\n","        MAX_FILE_SIZE=args.MAX_FILE_SIZE,\n","        batch_size=args.batch_size,\n","        lr=args.lr,\n","        hidden_size=args.hidden_size,\n","        encoder_layers=args.encoder_layers,\n","        decoder_layers=args.decoder_layers,\n","        encoder_heads=args.encoder_heads,\n","        decoder_heads=args.decoder_heads,\n","        encoder_ff_size=args.encoder_ff_size,\n","        decoder_ff_size=args.decoder_ff_size,\n","        encoder_dropout=args.encoder_dropout,\n","        decoder_dropout=args.decoder_dropout,\n","        device=args.device,\n","        lr_scheduler_type=args.lr_scheduler_type,\n","        warmup_steps=args.warmup_steps,\n","        early_stopping_patience=args.early_stopping_patience\n","    )\n","    trainer.train(args.epochs, args.saved_model_directory)\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"id":"XuJgNKqaK_A7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736621315195,"user_tz":-120,"elapsed":526,"user":{"displayName":"Franco Ryan","userId":"01207996925900706828"}},"outputId":"ba3d81d9-4b94-4f2a-9ed6-f51bdf861928"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing train.py\n"]}]},{"cell_type":"code","source":["!python train.py \\\n","  --lang1 \"english\" \\\n","  --lang2 \"juhoansi\" \\\n","  --data_directory \"/content/drive/MyDrive/Translation2/Data/english-juhoansi\" \\\n","  --reverse 0 \\\n","  --MAX_LENGTH 60 \\\n","  --MAX_FILE_SIZE 100000 \\\n","  --batch_size 8 \\\n","  --lr 0.0005 \\\n","  --hidden_size 256 \\\n","  --encoder_layers 3 \\\n","  --decoder_layers 3 \\\n","  --encoder_heads 8 \\\n","  --decoder_heads 8 \\\n","  --encoder_ff_size 512 \\\n","  --decoder_ff_size 512 \\\n","  --encoder_dropout 0.1 \\\n","  --decoder_dropout 0.1 \\\n","  --device \"cuda\" \\\n","  --epochs 50 \\\n","  --lr_scheduler_type \"linear\" \\\n","  --warmup_steps 500 \\\n","  --early_stopping_patience 45 \\\n","  --saved_model_directory \"/content/drive/MyDrive/Translation2/saved_models/transformer\"\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"21C3_dSORKVu","executionInfo":{"status":"ok","timestamp":1736621592319,"user_tz":-120,"elapsed":177572,"user":{"displayName":"Franco Ryan","userId":"01207996925900706828"}},"outputId":"16fb840f-ff50-4d40-c8c2-219b1a740048"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","Epoch: 0, Time: 8s, Estimated 392 sec remaining.\n","\tTraining Loss: 5.1024 | Validation Loss: 4.7867\n","\tBLEU: 0.67, CHRF: 5.48, METEOR: 9.81\n","\n","Epoch: 1, Time: 12s, Estimated 576 sec remaining.\n","\tTraining Loss: 4.4093 | Validation Loss: 4.4542\n","\tBLEU: 1.06, CHRF: 5.19, METEOR: 10.91\n","\n","Epoch: 2, Time: 16s, Estimated 752 sec remaining.\n","\tTraining Loss: 4.0317 | Validation Loss: 4.2944\n","\tBLEU: 2.11, CHRF: 12.05, METEOR: 13.95\n","\n","Epoch: 3, Time: 20s, Estimated 920 sec remaining.\n","\tTraining Loss: 3.6825 | Validation Loss: 4.2563\n","\tBLEU: 0.82, CHRF: 15.39, METEOR: 14.80\n","\n","Epoch: 4, Time: 23s, Estimated 1035 sec remaining.\n","\tTraining Loss: 3.3068 | Validation Loss: 4.2355\n","\tBLEU: 1.25, CHRF: 16.94, METEOR: 15.32\n","\n","Epoch: 5, Time: 25s, Estimated 1100 sec remaining.\n","\tTraining Loss: 2.9742 | Validation Loss: 4.1652\n","\tBLEU: 1.77, CHRF: 19.03, METEOR: 17.51\n","\n","Epoch: 6, Time: 28s, Estimated 1204 sec remaining.\n","\tTraining Loss: 2.6842 | Validation Loss: 4.1479\n","\tBLEU: 2.03, CHRF: 19.08, METEOR: 19.62\n","\n","Epoch: 7, Time: 33s, Estimated 1386 sec remaining.\n","\tTraining Loss: 2.3475 | Validation Loss: 4.2111\n","\tBLEU: 1.58, CHRF: 20.85, METEOR: 18.58\n","\n","Epoch: 8, Time: 36s, Estimated 1476 sec remaining.\n","\tTraining Loss: 2.0876 | Validation Loss: 4.2406\n","\tBLEU: 2.34, CHRF: 20.74, METEOR: 20.18\n","\n","Epoch: 9, Time: 39s, Estimated 1560 sec remaining.\n","\tTraining Loss: 1.8723 | Validation Loss: 4.2911\n","\tBLEU: 2.28, CHRF: 21.13, METEOR: 20.76\n","\n","Epoch: 10, Time: 41s, Estimated 1599 sec remaining.\n","\tTraining Loss: 1.6722 | Validation Loss: 4.3426\n","\tBLEU: 1.40, CHRF: 21.37, METEOR: 20.87\n","\n","Epoch: 11, Time: 45s, Estimated 1710 sec remaining.\n","\tTraining Loss: 1.4640 | Validation Loss: 4.3221\n","\tBLEU: 2.24, CHRF: 21.53, METEOR: 21.00\n","\n","Epoch: 12, Time: 49s, Estimated 1813 sec remaining.\n","\tTraining Loss: 1.3085 | Validation Loss: 4.4565\n","\tBLEU: 2.21, CHRF: 21.98, METEOR: 21.37\n","\n","Epoch: 13, Time: 52s, Estimated 1872 sec remaining.\n","\tTraining Loss: 1.1220 | Validation Loss: 4.5307\n","\tBLEU: 2.58, CHRF: 21.89, METEOR: 21.15\n","\n","Epoch: 14, Time: 55s, Estimated 1925 sec remaining.\n","\tTraining Loss: 0.9938 | Validation Loss: 4.6087\n","\tBLEU: 2.48, CHRF: 23.36, METEOR: 22.93\n","\n","Epoch: 15, Time: 57s, Estimated 1938 sec remaining.\n","\tTraining Loss: 0.8785 | Validation Loss: 4.7811\n","\tBLEU: 1.51, CHRF: 21.84, METEOR: 19.91\n","\n","Epoch: 16, Time: 61s, Estimated 2013 sec remaining.\n","\tTraining Loss: 0.7824 | Validation Loss: 4.8291\n","\tBLEU: 2.56, CHRF: 22.66, METEOR: 21.45\n","\n","Epoch: 17, Time: 65s, Estimated 2080 sec remaining.\n","\tTraining Loss: 0.6610 | Validation Loss: 4.9017\n","\tBLEU: 2.01, CHRF: 21.86, METEOR: 20.77\n","\n","Epoch: 18, Time: 68s, Estimated 2108 sec remaining.\n","\tTraining Loss: 0.5998 | Validation Loss: 4.9565\n","\tBLEU: 2.34, CHRF: 23.30, METEOR: 22.66\n","\n","Epoch: 19, Time: 71s, Estimated 2130 sec remaining.\n","\tTraining Loss: 0.5213 | Validation Loss: 5.1050\n","\tBLEU: 3.03, CHRF: 23.28, METEOR: 21.94\n","\n","Epoch: 20, Time: 74s, Estimated 2146 sec remaining.\n","\tTraining Loss: 0.4530 | Validation Loss: 5.1211\n","\tBLEU: 2.64, CHRF: 24.50, METEOR: 23.08\n","\n","Epoch: 21, Time: 78s, Estimated 2184 sec remaining.\n","\tTraining Loss: 0.4365 | Validation Loss: 5.2060\n","\tBLEU: 2.41, CHRF: 22.73, METEOR: 22.85\n","\n","Epoch: 22, Time: 81s, Estimated 2187 sec remaining.\n","\tTraining Loss: 0.4105 | Validation Loss: 5.1860\n","\tBLEU: 2.35, CHRF: 23.86, METEOR: 21.01\n","\n","Epoch: 23, Time: 84s, Estimated 2184 sec remaining.\n","\tTraining Loss: 0.3690 | Validation Loss: 5.1978\n","\tBLEU: 2.54, CHRF: 21.95, METEOR: 19.70\n","\n","Epoch: 24, Time: 87s, Estimated 2175 sec remaining.\n","\tTraining Loss: 0.3545 | Validation Loss: 5.3807\n","\tBLEU: 2.91, CHRF: 23.29, METEOR: 22.44\n","\n","Epoch: 25, Time: 90s, Estimated 2160 sec remaining.\n","\tTraining Loss: 0.2941 | Validation Loss: 5.3605\n","\tBLEU: 2.85, CHRF: 24.17, METEOR: 23.02\n","\n","Epoch: 26, Time: 94s, Estimated 2162 sec remaining.\n","\tTraining Loss: 0.3126 | Validation Loss: 5.6056\n","\tBLEU: 2.37, CHRF: 22.28, METEOR: 20.81\n","\n","Epoch: 27, Time: 97s, Estimated 2134 sec remaining.\n","\tTraining Loss: 0.2698 | Validation Loss: 5.5464\n","\tBLEU: 2.42, CHRF: 23.30, METEOR: 21.94\n","\n","Epoch: 28, Time: 100s, Estimated 2100 sec remaining.\n","\tTraining Loss: 0.2518 | Validation Loss: 5.5619\n","\tBLEU: 2.65, CHRF: 23.32, METEOR: 22.76\n","\n","Epoch: 29, Time: 103s, Estimated 2060 sec remaining.\n","\tTraining Loss: 0.2409 | Validation Loss: 5.7118\n","\tBLEU: 3.04, CHRF: 23.79, METEOR: 22.14\n","\n","Epoch: 30, Time: 107s, Estimated 2033 sec remaining.\n","\tTraining Loss: 0.2322 | Validation Loss: 5.7408\n","\tBLEU: 2.58, CHRF: 22.83, METEOR: 21.46\n","\n","Epoch: 31, Time: 110s, Estimated 1980 sec remaining.\n","\tTraining Loss: 0.2182 | Validation Loss: 5.6779\n","\tBLEU: 2.96, CHRF: 23.50, METEOR: 23.20\n","\n","Epoch: 32, Time: 113s, Estimated 1921 sec remaining.\n","\tTraining Loss: 0.2029 | Validation Loss: 5.8412\n","\tBLEU: 2.97, CHRF: 23.11, METEOR: 21.81\n","\n","Epoch: 33, Time: 116s, Estimated 1856 sec remaining.\n","\tTraining Loss: 0.2085 | Validation Loss: 5.8630\n","\tBLEU: 3.37, CHRF: 24.28, METEOR: 23.04\n","\n","Epoch: 34, Time: 119s, Estimated 1785 sec remaining.\n","\tTraining Loss: 0.1864 | Validation Loss: 5.9505\n","\tBLEU: 3.43, CHRF: 24.16, METEOR: 23.30\n","\n","Epoch: 35, Time: 123s, Estimated 1722 sec remaining.\n","\tTraining Loss: 0.1826 | Validation Loss: 6.0045\n","\tBLEU: 2.90, CHRF: 24.31, METEOR: 23.69\n","\n","Epoch: 36, Time: 126s, Estimated 1638 sec remaining.\n","\tTraining Loss: 0.1894 | Validation Loss: 5.9706\n","\tBLEU: 3.00, CHRF: 23.86, METEOR: 22.59\n","\n","Epoch: 37, Time: 129s, Estimated 1548 sec remaining.\n","\tTraining Loss: 0.1594 | Validation Loss: 5.9968\n","\tBLEU: 3.25, CHRF: 23.29, METEOR: 23.30\n","\n","Epoch: 38, Time: 132s, Estimated 1452 sec remaining.\n","\tTraining Loss: 0.1712 | Validation Loss: 6.0305\n","\tBLEU: 2.67, CHRF: 22.93, METEOR: 22.74\n","\n","Epoch: 39, Time: 136s, Estimated 1360 sec remaining.\n","\tTraining Loss: 0.1585 | Validation Loss: 6.1307\n","\tBLEU: 2.27, CHRF: 23.13, METEOR: 21.16\n","\n","Epoch: 40, Time: 139s, Estimated 1251 sec remaining.\n","\tTraining Loss: 0.1765 | Validation Loss: 6.1630\n","\tBLEU: 2.72, CHRF: 23.84, METEOR: 23.40\n","\n","Epoch: 41, Time: 142s, Estimated 1136 sec remaining.\n","\tTraining Loss: 0.1581 | Validation Loss: 6.1999\n","\tBLEU: 2.30, CHRF: 23.09, METEOR: 21.89\n","\n","Epoch: 42, Time: 145s, Estimated 1015 sec remaining.\n","\tTraining Loss: 0.1565 | Validation Loss: 6.2591\n","\tBLEU: 2.52, CHRF: 21.98, METEOR: 21.47\n","\n","Epoch: 43, Time: 148s, Estimated 888 sec remaining.\n","\tTraining Loss: 0.1418 | Validation Loss: 6.3264\n","\tBLEU: 3.01, CHRF: 24.25, METEOR: 24.35\n","\n","Epoch: 44, Time: 153s, Estimated 765 sec remaining.\n","\tTraining Loss: 0.1489 | Validation Loss: 6.4542\n","\tBLEU: 2.28, CHRF: 22.94, METEOR: 21.64\n","\n","Epoch: 45, Time: 156s, Estimated 624 sec remaining.\n","\tTraining Loss: 0.1299 | Validation Loss: 6.4474\n","\tBLEU: 2.46, CHRF: 23.85, METEOR: 22.06\n","\n","Epoch: 46, Time: 158s, Estimated 474 sec remaining.\n","\tTraining Loss: 0.1439 | Validation Loss: 6.2763\n","\tBLEU: 2.58, CHRF: 22.91, METEOR: 22.94\n","\n","Epoch: 47, Time: 161s, Estimated 322 sec remaining.\n","\tTraining Loss: 0.1420 | Validation Loss: 6.4320\n","\tBLEU: 2.20, CHRF: 23.91, METEOR: 22.70\n","\n","Epoch: 48, Time: 166s, Estimated 166 sec remaining.\n","\tTraining Loss: 0.1240 | Validation Loss: 6.3961\n","\tBLEU: 3.00, CHRF: 24.15, METEOR: 22.71\n","\n","Epoch: 49, Time: 169s, Estimated 0 sec remaining.\n","\tTraining Loss: 0.1276 | Validation Loss: 6.4607\n","\tBLEU: 2.26, CHRF: 22.87, METEOR: 20.46\n","\n","Training finished!\n","Evaluating on TEST set...\n","TEST Loss: 6.4809\n","TEST BLEU: 2.28, TEST CHRF: 22.48, TEST METEOR: 22.24\n"]}]},{"cell_type":"code","source":["%%writefile plot_metrics.py\n","\n","import json\n","import matplotlib.pyplot as plt\n","\n","def main(json_path):\n","    with open(json_path, \"r\") as f:\n","        data = json.load(f)\n","\n","    epochs = [item[\"epoch\"] for item in data if \"epoch\" in item]\n","    train_loss = [item[\"train_loss\"] for item in data if \"train_loss\" in item]\n","    bleu = [item[\"bleu\"] for item in data if \"bleu\" in item]\n","    chrf = [item[\"chrf\"] for item in data if \"chrf\" in item]\n","    meteor = [item[\"meteor\"] for item in data if \"meteor\" in item]\n","\n","    # 1) Plot train_loss\n","    plt.figure(figsize=(8,5))\n","    plt.plot(epochs, train_loss, marker=\"o\", label=\"Train Loss\", color=\"red\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.title(\"Training Loss Over Epochs\")\n","    plt.legend()\n","    plt.grid(True)\n","    plt.savefig(\"train_loss_over_epochs.png\")\n","    plt.show()\n","\n","    # 2) Plot BLEU\n","    plt.figure(figsize=(8,5))\n","    plt.plot(epochs, bleu, marker=\"o\", label=\"BLEU\", color=\"blue\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"BLEU Score\")\n","    plt.title(\"BLEU Score Over Epochs\")\n","    plt.legend()\n","    plt.grid(True)\n","    plt.savefig(\"bleu_over_epochs.png\")\n","    plt.show()\n","\n","    # 3) Plot CHRF\n","    plt.figure(figsize=(8,5))\n","    plt.plot(epochs, chrf, marker=\"o\", label=\"CHRF\", color=\"green\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"CHRF Score\")\n","    plt.title(\"CHRF Score Over Epochs\")\n","    plt.legend()\n","    plt.grid(True)\n","    plt.savefig(\"chrf_over_epochs.png\")\n","    plt.show()\n","\n","    # 4) Plot METEOR (optional)\n","    if meteor:\n","        plt.figure(figsize=(8,5))\n","        plt.plot(epochs[:len(meteor)], meteor, marker=\"o\", label=\"METEOR\", color=\"purple\")\n","        plt.xlabel(\"Epoch\")\n","        plt.ylabel(\"METEOR (%)\")\n","        plt.title(\"METEOR Score Over Epochs\")\n","        plt.legend()\n","        plt.grid(True)\n","        plt.savefig(\"meteor_over_epochs.png\")\n","        plt.show()\n","\n","if __name__ == \"__main__\":\n","    import sys\n","    if len(sys.argv) < 2:\n","        print(\"Usage: python plot_metrics.py metrics_history.json\")\n","        sys.exit(1)\n","    json_file = sys.argv[1]\n","    main(json_file)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FAwYn98-zuB6","executionInfo":{"status":"ok","timestamp":1736623197269,"user_tz":-120,"elapsed":503,"user":{"displayName":"Franco Ryan","userId":"01207996925900706828"}},"outputId":"9c03731f-282c-4464-ebdc-03c16b0c8c2e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing plot_metrics.py\n"]}]},{"cell_type":"code","source":["!python plot_metrics.py metrics_history.json\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aV4pYLHizxFx","executionInfo":{"status":"ok","timestamp":1736623206417,"user_tz":-120,"elapsed":2741,"user":{"displayName":"Franco Ryan","userId":"01207996925900706828"}},"outputId":"03037201-aa0b-4201-b4aa-c668f30428c6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Figure(800x500)\n","Figure(800x500)\n","Figure(800x500)\n","Figure(800x500)\n"]}]}]}